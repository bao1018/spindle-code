# -*- mode: python -*-

import os

DEBUG = True
TEMPLATE_DEBUG = DEBUG

ADMINS = (
    ('Your Name', 'Your Email'),
)

MANAGERS = ADMINS

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.', # Add 'postgresql_psycopg2', 'postgresql', 'mysql', 'sqlite3' or 'oracle'.
        'NAME': '',                      # Or path to database file if using sqlite3.
        'USER': '',                      # Not used with sqlite3.
        'PASSWORD': '',                  # Not used with sqlite3.
        'HOST': '',                      # Set to empty string for localhost. Not used with sqlite3.
        'PORT': '',                      # Set to empty string for default. Not used with sqlite3.
    }
}

# Make this unique, and don't share it with anybody.
SECRET_KEY = ''

EMAIL_HOST = ''
EMAIL_IMAP = ''
EMAIL_HOST_USER = ''
EMAIL_HOST_PASSWORD = ''
EMAIL_PORT = 587
EMAIL_USE_TLS = True
EMAIL_SUBJECT_PREFIX = ''

#----------------------------------------
# Spindle settings
#----------------------------------------

# URL of an RSS feed to scrape audio and video from
SPINDLE_SCRAPE_RSS_URL = ''

# New items added from RSS will be added to the transcription queue
# using the transcription engine defined below.  To disable this
# behavior, set it explicitly to None:
SPINDLE_DEFAULT_TRANSCRIPTION_ENGINE = 'spindle.transcribe.sphinx'
 
# The absolute path to the CMU Sphinx installation, so that
# SPINDLE_SPHINX_DIRECTORY/bin/Transcriber.jar is the appropriate .jar
# file
SPINDLE_SPHINX_DIRECTORY = ''

# A directory where Sphinx output and logging information will be saved
# If this is blank, will use a default temporary file directory
SPINDLE_SPHINX_OUTPUT_DIRECTORY = ''

# A public directory for published captions and transcripts
SPINDLE_PUBLIC_DIRECTORY = ''

# URL for published items
SPINDLE_PUBLIC_URL = ''

# Filename for the RSS feed of exported items
SPINDLE_EXPORTS_RSS_FILENAME = 'exports.rss'

# Log-likelihood cutoff below which keywords will be excluded from RSS
# export
SPINDLE_KEYWORD_LL_THRESHOLD = 40

# Authentication for the Koemei speech-to-text service (koemei.com),
# accessed from Spindle via the "spindle.transcribe.koemei"
# transcription engine.
SPINDLE_KOEMEI_USERNAME = ''
SPINDLE_KOEMEI_PASSWORD = ''

#----------------------------------------
# Celery configuration options
#----------------------------------------

# Spindle uses the Celery task queue framework heavily for offloading
# long-running tasks like importing and exporting files and running
# speech-to-text transcriptions.  See http://celeryproject.org/.  The
# default configuration given here should work, but it can be
# improved.

# By default, Celery will use the Django database as its message
# queue. This works OK for testing, but some operations like
# canceling/removing items from the queue will be buggy.  Using
# RabbitMQ as the message queue is more robust: see below.
BROKER_URL = 'django://'

# Uncomment and edit the line below to have Celery use RabbitMQ
# (http://www.rabbitmq.com) as its messaging system.  The
# "setup_rabbitmq.sh" in spindle/web may help with configuring
# RabbitMQ for Celery.

# BROKER_URL = 'amqp://user:password@localhost/spindle'

# If BROKER_URL is not left as the default 'django://', you must also
# configure a Celery result backend in order to monitor the status of
# running tasks.  See the celery documentation for information on the
# different options.  Using the same database as Django uses (the
# setup commented out below) seems to work fine.  Using the RabbitMQ
# message queue for results does not work well!

# Uncommment these lines to use the Django database for Celery
# result status.

# def dburi_from_config(database='default'):
#     conf = DATABASES[database]
#     engine = conf['ENGINE'].split('.')[-1]
#     if engine.startswith('sqlite'):
#         return 'sqlite://' + conf['NAME']
#     else:
#         if engine.startswith('postgres'): engine='postgresql'
#         if conf['HOST'] == '':
#             host = 'localhost'
#         else:
#             host = conf['HOST']
#         return u'{engine}://{user}:{pass}@{host}/{database}'.format(
#             engine = engine, user = conf['USER'], pass=conf['PASSWORD'],
#             host = host, database = conf['NAME'])

# CELERY_RESULT_BACKEND = 'database'
# CELERY_RESULT_DBURI = dburi_from_config()        
    

# Settings below this line should not need changing.

# Enable the Django-database-backed crontab scheduler, so that
# periodic tasks like feed importing can be configured from the Django
# admin interface.
CELERYBEAT_SCHEDULER="djcelery.schedulers.DatabaseScheduler"

# Spindle stores information about running Celery tasks in the Django
# cache.  A working cache backend must be configured for these
# operations, including importing and exporting files, to work
# properly.  The following setup should work fine without changes.
# When creating the database, you need to run "manage.py
# createcachetable django_cache_table" to set it up.
# 
CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.db.DatabaseCache',
        'LOCATION': 'django_cache_table',
    }
}
